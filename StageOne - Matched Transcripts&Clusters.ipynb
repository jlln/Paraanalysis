{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "StageOne.py - Take the clusters produced by Paralyzer, and identify transcripts which could contain those clusters."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "In this example series, input files and working directories are hardcoded. In practice, these are provided by the user as command line arguments when invoking the script."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##This script accepts Paralyzer output files (to be listed as arguments on the command line), and requires a \n",
      "##copy of UCSC_transcripts.txt to be in the directory.\n",
      "\n",
      "\n",
      "\n",
      "import pandas\n",
      "import numpy as np\n",
      "import os\n",
      "import cPickle as pickle\n",
      "\n",
      "import sys\n",
      "\n",
      "ucsc_db=pandas.read_csv(\"UCSC_transcripts.txt\",delim_whitespace=True,low_memory=False) # From the UCSC Database; a list of transcripts.\n",
      "ucsc_names=pandas.read_csv('UCSC_genenames.csv')                      #To get meaningful names for the ucsc transcriptIds.\n",
      "\n",
      "# input_filenames=sys.argv[1:]\n",
      "input_filenames=['MiSeqSFPQc.csv','MiSeqNONOc.csv']\n",
      "\n",
      "datasets=[]\n",
      "dataset_names=[]\n",
      "for f in input_filenames:\n",
      "    dataset_names.append(f[:16])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n_transcripts=len(ucsc_db)\n",
      "\n",
      "\n",
      "\n",
      "def match_check(transcript):\n",
      "    \n",
      "    if transcript['txStart']<clusterEnd:\n",
      "        if transcript['txEnd']>clusterStart:\n",
      "            transcripts.append(transcript['#name'])\n",
      "\n",
      "\n",
      "clusters_dict={}\n",
      "clusters_by_transcripts_dict={}\n",
      "matched_transcripts_dict={}\n",
      "\n",
      "#Establish Output Directory\n",
      "transcript_output_directory_name='_'.join(dataset_names)+\"Transcripts\"\n",
      "transcript_output_directory=\"./\"+transcript_output_directory_name+\"/\"\n",
      "if not os.path.exists(transcript_output_directory):\n",
      "    os.makedirs(transcript_output_directory)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/work/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1070: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
        "  data = self._reader.read(nrows)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'Chromosome', u'Strand', u'ClusterStart', u'ClusterEnd', u'ClusterID', u'ClusterSequence', u'ReadCount', u'ModeLocation', u'ModeScore', u'ConversionLocationCount', u'ConversionEventCount', u'NonConversionEventCount', u'FilterType'], dtype='object')\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "First identify which transcripts contain each cluster. This stage also produces a list of transcripts by their translation status, named Translation_Status.csv, and a list of the overall count, _Translation_Status_Totals.csv."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First identify which transcripts contain each cluster.\n",
      "\n",
      "for dataset in input_filenames:                                           \n",
      "    clusters=pandas.read_csv(dataset)\n",
      "    dataset=dataset[:16]\n",
      "    datasets.append(dataset)\n",
      "    clusters_dict[dataset]=clusters\n",
      "    if os.path.isfile(transcript_output_directory+\"%smatchedreads.csv\"%(dataset)) is True:               #checking if the dataset has already been processed,because this step is relatively slow\n",
      "        \n",
      "        clusters=pandas.read_csv(transcript_output_directory+\"%smatchedreads.csv\"%(dataset))\n",
      "        transcripts_list=clusters['transcripts'].tolist()\n",
      "        matched_transcripts=[]\n",
      "        for transcript in transcripts_list:\n",
      "            for g in transcript[1:-1].split(\", \"):\n",
      "                if len(g)>2:\n",
      "                    matched_transcripts.append(g[1:-1])\n",
      "        matched_transcripts=frozenset(matched_transcripts)\n",
      "        \n",
      "        \n",
      "    else:    \n",
      "        transcripts_list=[]\n",
      "        \n",
      "        \n",
      "        for cluster in clusters.iterrows():\n",
      "            transcripts=[]\n",
      "            c=cluster[1]\n",
      "            cluster_transcripts=[]\n",
      "            chromosome=c['Chromosome']\n",
      "            clusterStart=c['ClusterStart']\n",
      "            clusterEnd=c['ClusterEnd']\n",
      "            transcripts_on_read_chromosome=ucsc_db[ucsc_db['chrom']==chromosome]\n",
      "            possible_transcripts = transcript_on_read_chromosome[transcripts_on_read_chromosome['txStart']<clusterEnd]\n",
      "            possible_transcripts=possible_transcripts[possible_transcripts['txEnd']>clusterStart]\n",
      "            \n",
      "            transcripts_list.append(possible_transcripts.tolist())\n",
      "\n",
      "       \n",
      "        clusters['transcripts']=pandas.Series(transcripts_list)\n",
      "        \n",
      "        clusters.to_csv(transcript_output_directory+\"%smatchedreads.csv\"%(dataset))\n",
      "        matched_transcripts=[]\n",
      "        for l in transcripts_list:\n",
      "            for transcript in l:\n",
      "                matched_transcripts.append(transcript)\n",
      "        matched_transcripts=frozenset(matched_transcripts)\n",
      "        \n",
      "\n",
      "        \n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "#Second, do the reverse, ie identify the clusters that are contained within each transcript, and merge with the ucsc table. Store the resulting dataframe in a dicionary keyed to the dataset.\n",
      "    clusters_lists=[]\n",
      "\n",
      "    for transcript in matched_transcripts:\n",
      "        f=lambda x:transcript in x\n",
      "        mask=clusters['transcripts'].apply(f)\n",
      "        clusters_lists.append((clusters[mask]['ClusterID']).values)\n",
      "    \n",
      "    matched_transcripts=list(matched_transcripts)\n",
      "    \n",
      "    index_matched_transcripts=range(len(matched_transcripts))\n",
      "    read_counts=clusters['ReadCount']\n",
      "    \n",
      "        \n",
      "        \n",
      "    d={\"transcript\":matched_transcripts,\n",
      "\"reads\":clusters_lists}\n",
      "    matched_df=pandas.DataFrame(d,index=index_matched_transcripts)\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    ucsc_transcript_merge=pandas.merge(matched_df,ucsc_db,left_on='transcript',right_on=\"#name\",how=\"left\")\n",
      "    \n",
      "    \n",
      "    label=[dataset]*len(ucsc_transcript_merge)\n",
      "    ucsc_transcript_merge['dataset']=label\n",
      "\n",
      "    \n",
      "    clusters_by_transcripts_dict[dataset]=ucsc_transcript_merge\n",
      "    \n",
      "\n",
      "\n",
      "#Thirdly, make a table, which, for each transcript, lists the matched clusters from each dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "dataset_dfs_to_merge=[]\n",
      "column_names_in_merged_df=['transcript']\n",
      "transcripts_with_clusters=clusters_by_transcripts_dict[datasets[0]]\n",
      "colnames=transcripts_with_clusters.columns\n",
      "colnames=list(colnames)\n",
      "colnames[0]=datasets[0]\n",
      "transcripts_with_clusters.columns=colnames\n",
      "transcripts_with_clusters=transcripts_with_clusters[['transcript',datasets[0]]]\n",
      "\n",
      "for dataset in datasets[1:]:\n",
      "    dataset_clusters=clusters_by_transcripts_dict[dataset][['transcript','reads']]\n",
      "    dataset_clusters.columns=['transcript',dataset]\n",
      "    dataset_dfs_to_merge.append(dataset_clusters)\n",
      "    transcripts_with_clusters=pandas.merge(left=transcripts_with_clusters,right=dataset_clusters,on='transcript',how='outer')\n",
      "    \n",
      "transcripts_with_clusters_df=pandas.merge(left=transcripts_with_clusters,right=ucsc_db,left_on='transcript',right_on='#name',how='left')\n",
      "named_transcripts_with_clusters_df=pandas.merge(transcripts_with_clusters_df,ucsc_names,left_on=\"#name\",right_on=\"#kgID\",how=\"left\")\n",
      "\n",
      "\n",
      "\n",
      "named_transcripts_with_clusters_df.to_csv(transcript_output_directory+'named_transcripts_with_cluster.csv')\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "#Step Four: Produce a dataframe for each transcript containing clusters. 5 columns:clusterID,conversioneventcount,dataset,cluster_start,cluster_end. Only the nucleotides within clusters are described. Nucleotides are numbered by distance from transcription start.\n",
      "\n",
      "def retrieve_clusters(transcript):    \n",
      "\n",
      "    \n",
      "    transcript_name=transcript['#name']\n",
      "   \n",
      "    df_clusterID=[]\n",
      "    df_dataset=[]\n",
      "    df_conversioneventcount=[]\n",
      "    df_cluster_start=[]\n",
      "    df_cluster_end=[]\n",
      "\n",
      "    start_coord=transcript['txStart']\n",
      "    end_coord=transcript['txEnd']\n",
      "    for dataset in datasets:\n",
      "        \n",
      "        clusters=transcript[dataset]                #retrieve the clusters of that dataset(if any) matched to the transcript.\n",
      "        clusters_dataframe=clusters_dict[dataset]   #The original clusters dataset, from which the additional cluster data will be retrieved.\n",
      "\n",
      "        if type(clusters) is not float:             #Pandas considers empty cells to be floats\n",
      "            for clusterID in clusters:\n",
      "                cluster=clusters_dataframe[clusters_dataframe['ClusterID']==clusterID]\n",
      "                \n",
      "                conversion_event_count=cluster['ConversionEventCount'].values[0]\n",
      "                cluster_start=cluster['ClusterStart'].values[0]\n",
      "                cluster_end=cluster['ClusterEnd'].values[0]\n",
      "                df_clusterID.append(clusterID)\n",
      "                df_dataset.append(dataset)\n",
      "                df_conversioneventcount.append(conversion_event_count)\n",
      "                df_cluster_start.append(cluster_start-start_coord)\n",
      "                df_cluster_end.append(cluster_end-start_coord)\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "    \n",
      "\n",
      "    df_dict={'Dataset':df_dataset,\n",
      "    \"ClusterID\":df_clusterID,\n",
      "    \"ConversionEventCount\":df_conversioneventcount,\n",
      "    \"ClusterStart\":df_cluster_start,\n",
      "    \"ClusterEnd\":df_cluster_end}\n",
      "\n",
      "    df=pandas.DataFrame(df_dict)\n",
      "\n",
      "\n",
      "    output_path=transcript_output_directory+transcript_name+'.csv'\n",
      "    \n",
      "    df.to_csv(output_path)\n",
      "    \n",
      "    transcript_cluster_files.append(transcript_output_directory+transcript_name+'.csv')\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transcript_cluster_files=[]\n",
      "transcripts_with_clusters_df.apply(retrieve_clusters,axis=1)\n",
      "\n",
      "transcripts_reads_filenames={\"File\":transcript_cluster_files}\n",
      "transcripts_reads_filenames=pandas.DataFrame(transcripts_reads_filenames)\n",
      "\n",
      "transcripts_reads_filenames.to_csv(transcript_output_directory+'transcript_list.csv')\n",
      "\n",
      "\n",
      "\n",
      "#Step Five: Create initial annotations for each of the transcripts previously identified. Each transcript gets its own annotation csv file, naming is \n",
      "                                    #as above, but suffixed with 'annotation'.\n",
      "def create_annotations(transcript):\n",
      "  \n",
      "    feature_start=[]\n",
      "    feature_end=[]\n",
      "    feature_type=[]\n",
      "    \n",
      "    transcript_name=transcript['#name']\n",
      "    list_of_transcripts.append(transcript_name)\n",
      "    txStart=transcript['txStart']\n",
      "    txEnd=transcript['txEnd']\n",
      "    feature_start.append(txStart)\n",
      "    cdsStart=transcript['cdsStart']\n",
      "    cdsEnd=transcript['cdsEnd']\n",
      "\n",
      "    \n",
      "    if cdsStart==cdsEnd:\n",
      "        feature_end.append(txEnd)\n",
      "        feature_type.append(\"Untranslated Transcript\")\n",
      "        translated_untranslated_transcripts.append(transcript_name)\n",
      "        translated_untranslated_status.append(\"Untranslated\")\n",
      "        \n",
      "    else:\n",
      "        feature_end.append(cdsStart)\n",
      "        feature_type.append(\"5'-UTR\")\n",
      "        feature_start.append(cdsEnd)\n",
      "        feature_end.append(txEnd)\n",
      "        feature_type.append(\"3'-UTR\")\n",
      "        translated_untranslated_transcripts.append(transcript_name)\n",
      "        translated_untranslated_status.append(\"Translated\")\n",
      "        \n",
      "    n_exons=transcript['exonCount']\n",
      "\n",
      "    exon_starts=(transcript['exonStarts']).split(\",\")[:-1]\n",
      "    exon_starts[:]=[int(x) for x in exon_starts]\n",
      "      \n",
      "    \n",
      "    exon_ends=(transcript['exonEnds']).split(\",\")[:-1]\n",
      "    exon_ends[:]=[int(x) for x in exon_ends]\n",
      "      \n",
      "\n",
      "    feature_start.extend(exon_starts)\n",
      "    feature_end.extend(exon_ends)\n",
      "    feature_type.extend(['Exon']*n_exons)\n",
      "\n",
      "    \n",
      "    \n",
      "       \n",
      "        \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    d={'feature_start':feature_start,\n",
      "    'feature_end':feature_end,\n",
      "    'feature_type':feature_type}\n",
      "    df=pandas.DataFrame(d)\n",
      "  \n",
      "\n",
      "    \n",
      "    outputname=transcript_output_directory+transcript_name+'annotations'+'.csv'\n",
      "    df.to_csv(outputname)\n",
      "    annotation_filenames.append(outputname)\n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "\n",
      "annotation_filenames=[]\n",
      "list_of_transcripts=[]  #for the index in step six.\n",
      "#Two lists to keep track of transcript translation status: one list of transcript names and one list of translation statuses.\n",
      "translated_untranslated_transcripts=[]\n",
      "translated_untranslated_status=[]\n",
      "\n",
      "\n",
      "transcripts_with_clusters_df.apply(create_annotations,axis=1)\n",
      "\n",
      "\n",
      "\n",
      "# Step Six: Create csv file indexes to assist downstream scripts.\n",
      "cluster_and_transcript_annotation_file_index={\"Transcript_Clusters\":transcript_cluster_files,\n",
      "                                                \"Transcript_Annotation\":annotation_filenames}\n",
      "\n",
      "\n",
      "\n",
      "cluster_and_transcript_annotation_file_index=pandas.DataFrame(cluster_and_transcript_annotation_file_index)\n",
      "cluster_and_transcript_annotation_file_index.to_csv(transcript_output_directory+\"cluster_and_transcript_annotation_file_index.csv\")\n",
      "#Step Seven: ClusterCounts for untranslated and translated transcripts\n",
      "cluster_counts={}\n",
      "for d in datasets:\n",
      "    cluster_counts[d]=[]\n",
      "\n",
      "for transcript in translated_untranslated_transcripts:\n",
      "    for d in datasets:\n",
      "        if type(transcripts_with_clusters_df[transcripts_with_clusters_df[\"#name\"]==transcript][d].values[0]) is float:    #empty cell test; pandas considers empty cells as floats.\n",
      "            cluster_counts[d].append(0)\n",
      "        else:\n",
      "            \n",
      "            cluster_counts[d].append(len(transcripts_with_clusters_df[transcripts_with_clusters_df[\"#name\"]==transcript][d].values[0]))\n",
      "\n",
      "\n",
      "translation={\"Transcript\":translated_untranslated_transcripts,\"Translation_Status\":translated_untranslated_status}\n",
      "\n",
      "translation=pandas.DataFrame(translation)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for d in datasets:\n",
      "    translation[d]=pandas.Series(cluster_counts[d],index=translation.index)\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "translation.to_csv(transcript_output_directory+\"Translation_Status.csv\")\n",
      "\n",
      "grouped_by_translation_status=translation.groupby('Translation_Status')\n",
      "\n",
      "translation_status_summary=grouped_by_translation_status.aggregate(np.sum).reindex()\n",
      "\n",
      "\n",
      "translation_status_summary.to_csv(transcript_output_directory+\"_Translation_Status_Totals.csv\")\n",
      "\n",
      "\n",
      "\n",
      "print \"Output directory (provide as argument to StageTwo.py):\"\n",
      "print transcript_output_directory\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Output directory (provide as argument to StageTwo.py):\n",
        "./MiSeqSFPQc.csv_MiSeqNONOc.csvTranscripts/\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The script ends here, but we can take a look at the translation status totals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "This represents the number of translated and untranslated transcripts with the potential to contain the clusters of the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "translation.head()\n",
      "#Just the first five rows.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Transcript</th>\n",
        "      <th>Translation_Status</th>\n",
        "      <th>MiSeqSFPQc.csv</th>\n",
        "      <th>MiSeqNONOc.csv</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> uc010pwa.1</td>\n",
        "      <td> Translated</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> uc002sbj.3</td>\n",
        "      <td> Translated</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> uc010iub.3</td>\n",
        "      <td> Translated</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> uc001sut.4</td>\n",
        "      <td> Translated</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> uc010ugm.1</td>\n",
        "      <td> Translated</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "   Transcript Translation_Status  MiSeqSFPQc.csv  MiSeqNONOc.csv\n",
        "0  uc010pwa.1         Translated               2               0\n",
        "1  uc002sbj.3         Translated               2               1\n",
        "2  uc010iub.3         Translated               3               0\n",
        "3  uc001sut.4         Translated               1               0\n",
        "4  uc010ugm.1         Translated               1               0\n",
        "\n",
        "[5 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "translation_status_summary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>MiSeqSFPQc.csv</th>\n",
        "      <th>MiSeqNONOc.csv</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Translation_Status</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>Translated</th>\n",
        "      <td> 566</td>\n",
        "      <td> 462</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Untranslated</th>\n",
        "      <td> 266</td>\n",
        "      <td> 257</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>2 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "                    MiSeqSFPQc.csv  MiSeqNONOc.csv\n",
        "Translation_Status                                \n",
        "Translated                     566             462\n",
        "Untranslated                   266             257\n",
        "\n",
        "[2 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Transpose for plotting and normalize to percentages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "translation_summary=translation_status_summary.T\n",
      "translation_summary=translation_summary.div(translation_summary.sum(axis=1), axis=0)*100\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import vincent\n",
      "vincent.core.initialize_notebook()\n",
      "stack = vincent.StackedBar(translation_summary)\n",
      "stack.legend(title=\"Translation Status\")\n",
      "stack.colors(brew='Pastel1')\n",
      "stack.axis_titles(x=\"Sample\",y=\"Proportion of transcripts, by %\")\n",
      "stack.display()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "           <script>\n",
        "               \n",
        "                function vct_load_lib(url, callback){\n",
        "                      if(typeof d3 !== 'undefined' &&\n",
        "                         url === 'http://d3js.org/d3.v3.min.js'){\n",
        "                        callback()\n",
        "                      }\n",
        "                      var s = document.createElement('script');\n",
        "                      s.src = url;\n",
        "                      s.async = true;\n",
        "                      s.onreadystatechange = s.onload = callback;\n",
        "                      s.onerror = function(){\n",
        "                        console.warn(\"failed to load library \" + url);\n",
        "                        };\n",
        "                      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
        "                };\n",
        "                var vincent_event = new CustomEvent(\n",
        "                  \"vincent_libs_loaded\",\n",
        "                  {bubbles: true, cancelable: true}\n",
        "                );\n",
        "                \n",
        "               function load_all_libs(){\n",
        "                  console.log('Loading Vincent libs...')\n",
        "                  vct_load_lib('http://d3js.org/d3.v3.min.js', function(){\n",
        "                  vct_load_lib('http://d3js.org/d3.geo.projection.v0.min.js', function(){\n",
        "                  vct_load_lib('http://wrobstory.github.io/d3-cloud/d3.layout.cloud.js', function(){\n",
        "                  vct_load_lib('http://wrobstory.github.io/vega/vega.v1.3.3.js', function(){\n",
        "                  window.dispatchEvent(vincent_event);\n",
        "                  });\n",
        "                  });\n",
        "                  });\n",
        "                  });\n",
        "               };\n",
        "               if(typeof define === \"function\" && define.amd){\n",
        "                    if (window['d3'] === undefined ||\n",
        "                        window['topojson'] === undefined){\n",
        "                        require.config(\n",
        "                            {paths: {\n",
        "                              d3: 'http://d3js.org/d3.v3.min',\n",
        "                              topojson: 'http://d3js.org/topojson.v1.min'\n",
        "                              }\n",
        "                            }\n",
        "                          );\n",
        "                        require([\"d3\"], function(d3){\n",
        "                            console.log('Loading Vincent from require.js...')\n",
        "                            window.d3 = d3;\n",
        "                            require([\"topojson\"], function(topojson){\n",
        "                                window.topojson = topojson;\n",
        "                                load_all_libs();\n",
        "                            });\n",
        "                        });\n",
        "                    } else {\n",
        "                        load_all_libs();\n",
        "                    };\n",
        "               }else{\n",
        "                    console.log('Require.js not found, loading manually...')\n",
        "                    load_all_libs();\n",
        "               };\n",
        "\n",
        "           </script>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x108991450>"
       ]
      },
      {
       "html": [
        "<div id=\"vis707cfbc03f51411a998a1267358e0a51\"></div>\n",
        "<script>\n",
        "   ( function() {\n",
        "     var _do_plot = function() {\n",
        "       if (typeof vg === 'undefined') {\n",
        "         window.addEventListener('vincent_libs_loaded', _do_plot)\n",
        "         return;\n",
        "       }\n",
        "       vg.parse.spec({\"axes\": [{\"scale\": \"x\", \"title\": \"Sample\", \"type\": \"x\"}, {\"scale\": \"y\", \"title\": \"Proportion of transcripts, by %\", \"type\": \"y\"}], \"data\": [{\"name\": \"table\", \"values\": [{\"col\": \"Translated\", \"idx\": \"MiSeqSFPQc.csv\", \"val\": 68.02884615384616}, {\"col\": \"Untranslated\", \"idx\": \"MiSeqSFPQc.csv\", \"val\": 31.971153846153843}, {\"col\": \"Translated\", \"idx\": \"MiSeqNONOc.csv\", \"val\": 64.25591098748261}, {\"col\": \"Untranslated\", \"idx\": \"MiSeqNONOc.csv\", \"val\": 35.744089012517385}]}, {\"name\": \"stats\", \"source\": \"table\", \"transform\": [{\"keys\": [\"data.idx\"], \"type\": \"facet\"}, {\"type\": \"stats\", \"value\": \"data.val\"}]}], \"height\": 500, \"legends\": [{\"fill\": \"color\", \"offset\": 0, \"properties\": {}, \"title\": \"Translation Status\"}], \"marks\": [{\"from\": {\"data\": \"table\", \"transform\": [{\"keys\": [\"data.col\"], \"type\": \"facet\"}, {\"height\": \"data.val\", \"point\": \"data.idx\", \"type\": \"stack\"}]}, \"marks\": [{\"properties\": {\"enter\": {\"fill\": {\"field\": \"data.col\", \"scale\": \"color\"}, \"width\": {\"band\": true, \"offset\": -1, \"scale\": \"x\"}, \"x\": {\"field\": \"data.idx\", \"scale\": \"x\"}, \"y\": {\"field\": \"y\", \"scale\": \"y\"}, \"y2\": {\"field\": \"y2\", \"scale\": \"y\"}}}, \"type\": \"rect\"}], \"type\": \"group\"}], \"padding\": \"auto\", \"scales\": [{\"domain\": {\"data\": \"table\", \"field\": \"data.idx\"}, \"name\": \"x\", \"range\": \"width\", \"type\": \"ordinal\", \"zero\": false}, {\"domain\": {\"data\": \"stats\", \"field\": \"sum\"}, \"name\": \"y\", \"nice\": true, \"range\": \"height\"}, {\"domain\": {\"data\": \"table\", \"field\": \"data.col\"}, \"name\": \"color\", \"range\": [\"#fbb4ae\", \"#b3cde3\", \"#ccebc5\", \"#decbe4\", \"#fed9a6\", \"#ffffcc\", \"#e5d8bd\", \"#fddaec\", \"#f2f2f2\"], \"type\": \"ordinal\"}], \"width\": 960}, function(chart) {\n",
        "         chart({el: \"#vis707cfbc03f51411a998a1267358e0a51\"}).update();\n",
        "       });\n",
        "     };\n",
        "     _do_plot();\n",
        "   })();\n",
        "</script>\n",
        "<style>.vega canvas {width: 100%;}</style>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x101aef990>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}